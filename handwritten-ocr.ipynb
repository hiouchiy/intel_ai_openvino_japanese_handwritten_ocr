{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 日本語手紙文字OCRサンプル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINOのインストールディレクトリからオリジナルのサンプルコード関連ファイルをコピー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp $INTEL_OPENVINO_DIR/inference_engine/demos/python_demos/handwritten_japanese_recognition_demo/requirements.txt .\n",
    "!cp $INTEL_OPENVINO_DIR/inference_engine/demos/python_demos/handwritten_japanese_recognition_demo/models.lst .\n",
    "!cp -r $INTEL_OPENVINO_DIR/inference_engine/demos/python_demos/handwritten_japanese_recognition_demo/utils .\n",
    "!cp -r $INTEL_OPENVINO_DIR/inference_engine/demos/python_demos/handwritten_japanese_recognition_demo/data ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonライブラリをインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 事前学習済みモデルをダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/downloader.py --list models.lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ライブラリをインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import io\n",
    "import cv2\n",
    "import logging as log\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import PIL\n",
    "\n",
    "from openvino.inference_engine import IECore\n",
    "from utils.codec import CTCCodec\n",
    "\n",
    "import IPython.display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JapaneseHandwrittenOCR:\n",
    "    def __init__(self, model_path):\n",
    "        # Plugin initialization\n",
    "        ie = IECore()\n",
    "    \n",
    "        # Setup OpenVINO's IE\n",
    "        model = model_path\n",
    "        net = ie.read_network(model, os.path.splitext(model)[0] + \".bin\")\n",
    "        self.input_blob = next(iter(net.input_info))\n",
    "        self.out_blob = next(iter(net.outputs))\n",
    "        self.input_batch_size, self.input_channel, self.input_height, self.input_width = net.input_info[self.input_blob].input_data.shape\n",
    "        self.exec_net = ie.load_network(network=net, device_name=\"CPU\")\n",
    "        \n",
    "        # Setup codec\n",
    "        self.codec = CTCCodec(self.__get_characters__(), None, 20)\n",
    "    \n",
    "    def __get_characters__(self):\n",
    "        '''Get characters'''\n",
    "        charlist = \"data/kondate_nakayosi_char_list.txt\"\n",
    "        with open(charlist, 'r', encoding='utf-8') as f:\n",
    "            return ''.join(line.strip('\\n') for line in f)\n",
    "\n",
    "    def __preprocess_input__(self, image_name, height, width):\n",
    "        src = cv2.imread(image_name, cv2.IMREAD_GRAYSCALE)\n",
    "        ratio = float(src.shape[1]) / float(src.shape[0])\n",
    "        tw = int(height * ratio)\n",
    "        rsz = cv2.resize(src, (tw, height), interpolation=cv2.INTER_AREA).astype(np.float32)\n",
    "        # [h,w] -> [c,h,w]\n",
    "        img = rsz[None, :, :]\n",
    "        _, h, w = img.shape\n",
    "        # right edge padding\n",
    "        pad_img = np.pad(img, ((0, 0), (0, height - h), (0, width -  w)), mode='edge')\n",
    "        return pad_img\n",
    "    \n",
    "    def infer(self, image_path):\n",
    "        # Read and pre-process input image (NOTE: one image only)\n",
    "        input_path = image_path\n",
    "        input_image = self.__preprocess_input__(input_path, height=self.input_height, width=self.input_width)[None,:,:,:]\n",
    "        \n",
    "        preds = self.exec_net.infer(inputs={self.input_blob: input_image})\n",
    "        result = self.codec.decode(preds[self.out_blob])\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"data/test.png\"\n",
    "\n",
    "img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "f = io.BytesIO()\n",
    "PIL.Image.fromarray(img).save(f, 'jpeg')\n",
    "IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "ocr = JapaneseHandwrittenOCR(\"intel/handwritten-japanese-recognition-0001/FP32/handwritten-japanese-recognition-0001.xml\")\n",
    "result = ocr.infer(image_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO™ Model Serverを利用\n",
    "ここからは手書き文字認識モデルをOpenVINO Model Serverでマイクロサービス化して外出しにします。Model ServerとはgRPCを介してコミュニケーションを取ります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO Model Serverのセットアップ\n",
    "OpenVINO Model ServerをホストOS上で稼働させる手順です。こちらを実行後に以降の作業を進めてください。\n",
    "1. Model ServerのDockerイメージをダウンロード\n",
    "```Bash\n",
    "docker pull openvino/model_server:latest\n",
    "```\n",
    "1. 手書き文字認識用モデル（XMLファイルとBINファイル）を適当なフォルダへ格納\n",
    "1. Model Serverを起動\n",
    "```Bash\n",
    "docker run -d --rm -v C:\\Users\\hiroshi\\model\\ocr:/models/ocr/1 -p 9000:9000 openvino/model_server:latest --model_path /models/ocr --model_name ocr --port 9000 --log_level DEBUG  --shape auto\n",
    "```\n",
    "※\"C:\\Users\\hiroshi\\model\\ocr\"にモデルが格納されているとした場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モジュールのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-serving-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "import grpc\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "from tensorflow import make_tensor_proto, make_ndarray\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc, get_model_metadata_pb2\n",
    "\n",
    "from PIL import Image\n",
    "import PIL\n",
    "\n",
    "from utils.codec import CTCCodec\n",
    "\n",
    "import IPython.display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class RemoteJapaneseHandwrittenOCR:\n",
    "    def __init__(self, grpc_address='localhost', grpc_port=9000, model_name='ocr', model_version=None):\n",
    "        \n",
    "        #Settings for accessing model server\n",
    "        self.grpc_address = grpc_address\n",
    "        self.grpc_port = grpc_port\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        channel = grpc.insecure_channel(\"{}:{}\".format(self.grpc_address, self.grpc_port))\n",
    "        self.stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "        \n",
    "        # Get input shape info from Model Server\n",
    "        self.input_name, input_shape, self.output_name, output_shape = self.__get_input_name_and_shape__()\n",
    "        self.input_height = input_shape[2]\n",
    "        self.input_width = input_shape[3]\n",
    "        \n",
    "        # Setup codec\n",
    "        self.codec = CTCCodec(self.__get_characters__(), None, 20)\n",
    "    \n",
    "    def __get_input_name_and_shape__(self):\n",
    "        metadata_field = \"signature_def\"\n",
    "        request = get_model_metadata_pb2.GetModelMetadataRequest()\n",
    "        request.model_spec.name = self.model_name\n",
    "        if self.model_version is not None:\n",
    "            request.model_spec.version.value = self.model_version\n",
    "        request.metadata_field.append(metadata_field)\n",
    "\n",
    "        result = self.stub.GetModelMetadata(request, 10.0) # result includes a dictionary with all model outputs\n",
    "        input_metadata, output_metadata = self.__get_input_and_output_meta_data__(result)\n",
    "        input_blob = next(iter(input_metadata.keys()))\n",
    "        output_blob = next(iter(output_metadata.keys()))\n",
    "        return input_blob, input_metadata[input_blob]['shape'], output_blob, output_metadata[output_blob]['shape']\n",
    "    \n",
    "    def __get_input_and_output_meta_data__(self, response):\n",
    "        signature_def = response.metadata['signature_def']\n",
    "        signature_map = get_model_metadata_pb2.SignatureDefMap()\n",
    "        signature_map.ParseFromString(signature_def.value)\n",
    "        serving_default = signature_map.ListFields()[0][1]['serving_default']\n",
    "        serving_inputs = serving_default.inputs\n",
    "        input_blobs_keys = {key: {} for key in serving_inputs.keys()}\n",
    "        tensor_shape = {key: serving_inputs[key].tensor_shape\n",
    "                        for key in serving_inputs.keys()}\n",
    "        for input_blob in input_blobs_keys:\n",
    "            inputs_shape = [d.size for d in tensor_shape[input_blob].dim]\n",
    "            tensor_dtype = serving_inputs[input_blob].dtype\n",
    "            input_blobs_keys[input_blob].update({'shape': inputs_shape})\n",
    "            input_blobs_keys[input_blob].update({'dtype': tensor_dtype})\n",
    "        \n",
    "        serving_outputs = serving_default.outputs\n",
    "        output_blobs_keys = {key: {} for key in serving_outputs.keys()}\n",
    "        tensor_shape = {key: serving_outputs[key].tensor_shape\n",
    "                        for key in serving_outputs.keys()}\n",
    "        for output_blob in output_blobs_keys:\n",
    "            outputs_shape = [d.size for d in tensor_shape[output_blob].dim]\n",
    "            tensor_dtype = serving_outputs[output_blob].dtype\n",
    "            output_blobs_keys[output_blob].update({'shape': outputs_shape})\n",
    "            output_blobs_keys[output_blob].update({'dtype': tensor_dtype})\n",
    "\n",
    "        return input_blobs_keys, output_blobs_keys\n",
    "    \n",
    "    def __get_characters__(self):\n",
    "        '''Get characters'''\n",
    "        charlist = \"data/kondate_nakayosi_char_list.txt\"\n",
    "        with open(charlist, 'r', encoding='utf-8') as f:\n",
    "            return ''.join(line.strip('\\n') for line in f)\n",
    "\n",
    "    def __preprocess_input__(self, image_name, height, width):\n",
    "        src = cv2.imread(image_name, cv2.IMREAD_GRAYSCALE)\n",
    "        ratio = float(src.shape[1]) / float(src.shape[0])\n",
    "        tw = int(height * ratio)\n",
    "        rsz = cv2.resize(src, (tw, height), interpolation=cv2.INTER_AREA).astype(np.float32)\n",
    "        # [h,w] -> [c,h,w]\n",
    "        img = rsz[None, :, :]\n",
    "        _, h, w = img.shape\n",
    "        # right edge padding\n",
    "        pad_img = np.pad(img, ((0, 0), (0, height - h), (0, width -  w)), mode='edge')\n",
    "        return pad_img\n",
    "    \n",
    "    def infer(self, image_path):\n",
    "        # Read and pre-process input image (NOTE: one image only)\n",
    "        input_path = image_path\n",
    "        input_image = self.__preprocess_input__(input_path, height=self.input_height, width=self.input_width)[None,:,:,:]\n",
    "        input_image = input_image.astype(np.float32)\n",
    "        \n",
    "        # Model ServerにgRPCでアクセスしてモデルをコール\n",
    "        request = predict_pb2.PredictRequest()\n",
    "        request.model_spec.name = self.model_name\n",
    "        request.inputs[self.input_name].CopyFrom(make_tensor_proto(input_image, shape=(input_image.shape)))\n",
    "        result = self.stub.Predict(request, 10.0) # result includes a dictionary with all model outputs\n",
    "        preds = make_ndarray(result.outputs[self.output_name])\n",
    "        result = self.codec.decode(preds)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"data/test.png\"\n",
    "\n",
    "img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "f = io.BytesIO()\n",
    "PIL.Image.fromarray(img).save(f, 'jpeg')\n",
    "IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "ocr = RemoteJapaneseHandwrittenOCR(grpc_address='192.168.145.33', grpc_port='9000', model_name='ocr')\n",
    "result = ocr.infer(image_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# おしまい！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
